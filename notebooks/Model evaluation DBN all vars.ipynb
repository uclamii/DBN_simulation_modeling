{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = pd.read_csv(\n",
    "        # \"../Data/genie_datasets/DBN_predictions/all_var_model/cure_ckd_egfr_registry_preprocessed_project_preproc_data_discretized_UCLA_train_with_DBN_predictions.csv\",\n",
    "        \"../Data/genie_datasets/DBN_predictions/all_var_dbn_model/cure_ckd_egfr_registry_preprocessed_project_preproc_data_discretized_UCLA_valid_with_DBN_predictions.csv\",\n",
    "        # \"../Data/genie_datasets/DBN_predictions/all_var_model/cure_ckd_egfr_registry_preprocessed_project_preproc_data_discretized_UCLA_test_with_DBN_predictions.csv\",\n",
    "#         \"../Data/genie_datasets/DBN_predictions/all_var_model/Prov_cure_ckd_egfr_registry_preprocessed_project_preproc_data_discretized_using_UCLA_discritizer_with_DBN_predictions.csv\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate metrics over varying threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0,1,100)\n",
    "num_of_epochs = 5\n",
    "\n",
    "epoch_stats = dict()\n",
    "\n",
    "# looping over epochs\n",
    "for epoch_num in tqdm(range(0, num_of_epochs)):\n",
    "    \n",
    "    # epoch metrics dict\n",
    "    epoch_cm_metrics, epoch_metrics = dict(), dict()\n",
    "    \n",
    "    # target variable\n",
    "    target = \"year\" + str(1 + epoch_num) + \"_reduction_40_ge\"\n",
    "    truth = df_valid[target].str.replace(\"S_\", \"\").astype(int).values\n",
    "    predictions = df_valid[\"predictions_year\" + str(epoch_num + 1)]\n",
    "    tns, fps, fns, tps = [], [], [], []\n",
    "    precisions, recalls, specificities, f1_scores = [], [], [], []\n",
    "    \n",
    "    # looping over thresholds\n",
    "    for threshold in thresholds:\n",
    "        preds = (predictions>threshold)*1\n",
    "        tn, fp, fn, tp = confusion_matrix(truth, preds).ravel()\n",
    "        \n",
    "        # confusion matrices\n",
    "        tns.append(tn)\n",
    "        fps.append(fp)\n",
    "        fns.append(fn)\n",
    "        tps.append(tp)\n",
    "        \n",
    "        precision = precision_score(truth,preds)\n",
    "        recall = recall_score(truth,preds)\n",
    "        specificity = float(tn) / float(tn+fp)\n",
    "        f1Score = f1_score(truth, preds)\n",
    "        \n",
    "        # metrics\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        specificities.append(specificity)\n",
    "        f1_scores.append(f1Score)\n",
    "    \n",
    "    epoch_cm_metrics[\"TNs\"] = tns\n",
    "    epoch_cm_metrics[\"FPs\"] = fps\n",
    "    epoch_cm_metrics[\"FNs\"] = fns\n",
    "    epoch_cm_metrics[\"TPs\"] = tps\n",
    "    \n",
    "    epoch_metrics[\"Precision/PPV\"] = precisions\n",
    "    epoch_metrics[\"Recall/Sensitivity\"] = recalls\n",
    "    epoch_metrics[\"Specificity\"] = specificities\n",
    "    epoch_metrics[\"F1 score\"] = f1_scores\n",
    "    \n",
    "    epoch_stats[\"epoch_\" + str(epoch_num + 1)] = [epoch_cm_metrics] + [epoch_metrics]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "for epoch_num in range(1,6):\n",
    "    \n",
    "    names = list(epoch_stats[\"epoch_\" + str(epoch_num)][1].keys())\n",
    "    \n",
    "    # Create traces\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for name in names:\n",
    "        fig.add_trace(go.Scatter(x=thresholds, \n",
    "                                 y=epoch_stats[\"epoch_\" + str(epoch_num)][1][name],\n",
    "                                 mode='lines+markers',\n",
    "                                 name=name))\n",
    "\n",
    "    # Edit the layout\n",
    "    fig.update_layout(title='Metrics Vs Threshold epoch ' + str(epoch_num),\n",
    "                       xaxis_title='Thresholds',\n",
    "                       yaxis_title='Metric score [0,1]')\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot CM metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "for epoch_num in range(1,6):\n",
    "    \n",
    "    names = list(epoch_stats[\"epoch_\" + str(epoch_num)][0].keys())\n",
    "    \n",
    "    # Create traces\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for name in names:\n",
    "        fig.add_trace(go.Scatter(x=thresholds, \n",
    "                                 y=epoch_stats[\"epoch_\" + str(epoch_num)][0][name],\n",
    "                                 mode='lines+markers',\n",
    "                                 name=name))\n",
    "\n",
    "    # Edit the layout\n",
    "    fig.update_layout(title='CM Metrics Vs Threshold epoch ' + str(epoch_num),\n",
    "                       xaxis_title='Thresholds',\n",
    "                       yaxis_title='Metric count')\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation after choosing threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filenames = [\n",
    "        \"../Data/genie_datasets/DBN_predictions/all_var_model/cure_ckd_egfr_registry_preprocessed_project_preproc_data_discretized_UCLA_train_with_DBN_predictions.csv\",\n",
    "        \"../Data/genie_datasets/DBN_predictions/all_var_model/cure_ckd_egfr_registry_preprocessed_project_preproc_data_discretized_UCLA_valid_with_DBN_predictions.csv\",\n",
    "        \"../Data/genie_datasets/DBN_predictions/all_var_model/cure_ckd_egfr_registry_preprocessed_project_preproc_data_discretized_UCLA_test_with_DBN_predictions.csv\",\n",
    "        \"../Data/genie_datasets/DBN_predictions/all_var_model/Prov_cure_ckd_egfr_registry_preprocessed_project_preproc_data_discretized_using_UCLA_discritizer_with_DBN_predictions.csv\"\n",
    "]\n",
    "\n",
    "df_all_results = pd.DataFrame()\n",
    "\n",
    "\n",
    "for filename in tqdm(filenames):\n",
    "    \n",
    "    df_valid = pd.read_csv(filename, low_memory=False)\n",
    "    \n",
    "    filename = filename.replace(\"../Data/genie_datasets/DBN_predictions/all_var_model/cure_ckd_egfr_registry_preprocessed_project_preproc_data_discretized_\",\"\")\\\n",
    "                        .replace(\"../Data/genie_datasets/DBN_predictions/all_var_model/\",\"\")\\\n",
    "    \n",
    "    print(filename.split('/')[-1])\n",
    "    \n",
    "    num_of_epochs = 5\n",
    "    \n",
    "    cols = [\"Dataset\",\"Metric\"]+[\"Prediction Year \" + str(epoch_num + 1) + \", target year \" + str(epoch_num_targ+1) \\\n",
    "                                 for epoch_num in range(num_of_epochs) for epoch_num_targ in range(num_of_epochs) \\\n",
    "                      if epoch_num <= epoch_num_targ]\n",
    "    \n",
    "    auc_rocs, ap_aucs = [filename,\"AUC ROC\"],[filename,\"AP\"]\n",
    "    \n",
    "    tns, fps, fns, tps = [filename,\"TNs\"],[filename,\"FPs\"],[filename,\"FNs\"],[filename,\"TPs\"]\n",
    "    precisions, recalls = [filename,\"Precision/PPV\"],[filename,\"Recall/Sensitivity\"]\n",
    "    specificities, f1Scores = [filename,\"Specificity\"],[filename,\"F1 score\"]\n",
    "    \n",
    "    \n",
    "    filenames_epochs = []\n",
    "\n",
    "    # looping over epochs\n",
    "    for epoch_num in tqdm(range(0, num_of_epochs)):\n",
    "        for epoch_num_targ in range(0, num_of_epochs):\n",
    "            if epoch_num <= epoch_num_targ:\n",
    "                print(filename)\n",
    "                print(\"Prediction Year \" + str(epoch_num + 1) + \", target year \" + str(epoch_num_targ+1))\n",
    "                # TODO: for loop for target\n",
    "\n",
    "                # target variable\n",
    "                target = \"year\" + str(1 + epoch_num_targ) + \"_reduction_40_ge\"\n",
    "                truth = df_valid[target].str.replace(\"S_\", \"\").astype(int).values\n",
    "                predictions = df_valid[\"predictions_year\" + str(epoch_num + 1)]\n",
    "\n",
    "                auc_roc = roc_auc_score(truth,predictions)\n",
    "                ap_score = average_precision_score(truth,predictions)\n",
    "                auc_rocs.append(auc_roc)\n",
    "                ap_aucs.append(ap_score)\n",
    "\n",
    "                # testing optimal threshold\n",
    "                threshold = 0.14\n",
    "                preds = (predictions>threshold)*1\n",
    "                tn, fp, fn, tp = confusion_matrix(truth, preds).ravel()\n",
    "\n",
    "                tns.append(tn)\n",
    "                fps.append(fp) \n",
    "                fns.append(fn) \n",
    "                tps.append(tp)\n",
    "\n",
    "                # aucs\n",
    "                print(\"AUC ROC: \")\n",
    "                print(auc_roc)\n",
    "                print(\"AP: \")\n",
    "                print(ap_score)\n",
    "\n",
    "                # confusion matrices\n",
    "                print(\"TN, FP, FN, TP :\")\n",
    "                print(tn, fp, fn, tp)\n",
    "\n",
    "                precision = precision_score(truth,preds)\n",
    "                recall = recall_score(truth,preds)\n",
    "                specificity = float(tn) / float(tn+fp)\n",
    "                f1Score = f1_score(truth, preds)\n",
    "\n",
    "                precisions.append(precision)\n",
    "                recalls.append(recall) \n",
    "                specificities.append(specificity) \n",
    "                f1Scores.append(f1Score)\n",
    "\n",
    "                # metrics\n",
    "                print(\"Precision, Recall, Spec, F1 score :\")\n",
    "                print(precision,recall,specificity,f1Score)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    metrics = [auc_rocs, ap_aucs, tns, fps, fns, tps, precisions, recalls, specificities, f1Scores]\n",
    "    df_results = pd.DataFrame(data=metrics,columns=cols)\n",
    "        \n",
    "    df_all_results = df_all_results.append(df_results)\n",
    "    \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_results.to_csv(\"../Data/genie_datasets/DBN_predictions/Results/full_model_results.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
