{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "%load_ext blackcellmagic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## READ results\n",
    "\n",
    "results_path = \"../Data/genie_datasets/DBN_predictions/Results/\"\n",
    "\n",
    "models_results_filepaths = [filepath for filepath in glob.glob(results_path + \"*.csv\") if \"best\" not in filepath]\n",
    "\n",
    "models_results_filepath = models_results_filepaths[1]\n",
    "print(models_results_filepath)\n",
    "\n",
    "model_name = models_results_filepath.replace(results_path, \"\").replace(\n",
    "    \"all_datasets_results.csv\", \"\"\n",
    ")\n",
    "model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering out results\n",
    "\n",
    "- keeping only test set resulst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.read_csv(models_results_filepath)\n",
    "df_results = df_results[\n",
    "    df_results[\"Dataset\"].str.contains(\"_test_tested_on\")\n",
    "].reset_index(drop=True)\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting\n",
    "#  each years predictions to find the best model\n",
    "# for each site and filter\n",
    "\n",
    "num_epochs = 6\n",
    "metrics = [\"AP\", \"AUC ROC\"]\n",
    "results = []\n",
    "\n",
    "sites = [\"UCLA\", \"PSJH\", \"Combined\"]\n",
    "\n",
    "# for site in sites\n",
    "site = sites[0]\n",
    "\n",
    "results_path = \"../Data/genie_datasets/DBN_predictions/Results/\"\n",
    "\n",
    "sites_models_results_filepaths = [\n",
    "    model_path for model_path in glob.glob(results_path + \"*\") if site in model_path and \"best\" not in model_path\n",
    "]\n",
    "\n",
    "\n",
    "for models_results_filepath in sites_models_results_filepaths:\n",
    "    model_name = models_results_filepath.replace(results_path, \"\").replace(\n",
    "        \"all_datasets_results.csv\", \"\"\n",
    "    )\n",
    "\n",
    "    df_results = pd.read_csv(models_results_filepath)\n",
    "    df_results = df_results[\n",
    "        df_results[\"Dataset\"].str.contains(\"_test_tested_on\")\n",
    "    ].reset_index(drop=True)\n",
    "\n",
    "    filters = df_results[\"Race Ethnicity category\"].unique().tolist()\n",
    "\n",
    "    for fitler_cat in filters:\n",
    "        for metric in metrics:\n",
    "            # metric =\n",
    "\n",
    "            df_results_filter_group = df_results[\n",
    "                df_results[\"Race Ethnicity category\"] == fitler_cat\n",
    "            ].reset_index(drop=True)\n",
    "\n",
    "            # grouping into a d x d df of time\n",
    "            pred_target_res = []\n",
    "\n",
    "            for pred_year in range(1, num_epochs + 1):\n",
    "                pred_year_res = []\n",
    "                for target_year in range(1, num_epochs + 1):\n",
    "                    if pred_year == target_year:\n",
    "                        pred_year_res.append(\n",
    "                            df_results_filter_group.loc[\n",
    "                                df_results_filter_group[\"Metric\"] == metric,\n",
    "                                \"Prediction Year \"\n",
    "                                + str(pred_year)\n",
    "                                + \", target year \"\n",
    "                                + str(target_year),\n",
    "                            ].values[0]\n",
    "                        )\n",
    "                    else:\n",
    "                        pass\n",
    "                        # pred_year_res.append(np.nan)\n",
    "                pred_target_res.append(pred_year_res)\n",
    "            results.append(\n",
    "                [model_name, fitler_cat, metric]\n",
    "                + list(np.ravel(pred_target_res))\n",
    "                + [np.nanmean((np.ravel(pred_target_res)))]\n",
    "            )\n",
    "#saving into a dataframe\n",
    "annual_cols = [\"Year \" + str(epoch) for epoch in range(1, num_epochs + 1)]\n",
    "cols = [\"Model_site\", \"Filter\", \"Metric\"] + annual_cols + [\"Average metric\"]\n",
    "df_models_results = pd.DataFrame(results, columns=cols)\n",
    "\n",
    "# for each filter and for each metric\n",
    "# count how many times a model had the max performance\n",
    "# also compute the average performance over the years\n",
    "\n",
    "for fitler_cat in filters:\n",
    "    for metric in metrics:\n",
    "        # over annual metrics\n",
    "        # cond\n",
    "        cond = (df_models_results[\"Metric\"] == metric) & (\n",
    "            df_models_results[\"Filter\"] == fitler_cat\n",
    "        )\n",
    "        max_values = df_models_results.loc[cond, annual_cols].max().values\n",
    "        max_value_counts = np.zeros(df_models_results.loc[cond, annual_cols].shape)\n",
    "        for max_value in max_values:\n",
    "            # print((df_models_results.loc[df_models_results[\"Metric\"]==metric,annual_cols] == max_value))\n",
    "            max_value_counts = max_value_counts + (\n",
    "                (df_models_results.loc[cond, annual_cols] == max_value).values * 1\n",
    "            )\n",
    "        #  print(max_value_counts)\n",
    "        max_value_counts = np.sum(max_value_counts, axis=1)\n",
    "        #   print(max_value_counts)\n",
    "        df_models_results.loc[\n",
    "            cond, \"Counts of years of max performance\"\n",
    "        ] = max_value_counts\n",
    "        ##########\n",
    "        # average metric\n",
    "        avg_max_value = df_models_results.loc[cond, \"Average metric\"].max()\n",
    "        avg_max_value_bin = (\n",
    "            df_models_results.loc[cond, \"Average metric\"] == avg_max_value\n",
    "        ).values * 1\n",
    "\n",
    "        df_models_results.loc[cond, \"Max Average metric flag\"] = avg_max_value_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# focusing on the best models best on average performance\n",
    "# per metric and filter for each site\n",
    "# saving best models in a dataframe based on the average metric\n",
    "df_best_models = pd.DataFrame()\n",
    "for fitler_cat in filters:\n",
    "    for metric in metrics:\n",
    "        cond = (df_models_results[\"Metric\"] == metric) & (\n",
    "            df_models_results[\"Filter\"] == fitler_cat\n",
    "        )\n",
    "        # by average metric\n",
    "        df_models_results_cond = df_models_results[cond].reset_index(drop=True)\n",
    "        cond_avg_max = (\n",
    "            df_models_results_cond[\"Max Average metric flag\"]\n",
    "            == df_models_results_cond[\"Max Average metric flag\"].max()\n",
    "        )\n",
    "        df_best_models = df_best_models.append(df_models_results_cond[cond_avg_max])\n",
    "df_best_models = df_best_models.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', 500)\n",
    "# df_models_results.sort_values(by=[\"Filter\",\"Metric\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results[\"Dataset\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot results by filter based on metric and best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import os\n",
    "\n",
    "# Selecting\n",
    "#  each years predictions to find the best model\n",
    "# for each site and filter\n",
    "\n",
    "num_epochs = 6\n",
    "metrics = [\"AP\", \"AUC ROC\"]\n",
    "results = []\n",
    "\n",
    "sites = [\"UCLA\", \"PSJH\", \"Combined\"]\n",
    "\n",
    "# for site in sites\n",
    "# site = sites[1]\n",
    "\n",
    "results_path = \"../Data/genie_datasets/DBN_predictions/Results/\"\n",
    "\n",
    "sites_models_results_filepaths = [\n",
    "    model_path for model_path in glob.glob(results_path + \"*\") if site in model_path\n",
    "]\n",
    "\n",
    "print(sites_models_results_filepaths)\n",
    "\n",
    "model_names_filter_groups = df_best_models[df_best_models[\"Metric\"] == \"AP\"][\n",
    "    [\"Model_site\", \"Filter\"]\n",
    "].reset_index(drop=True)\n",
    "\n",
    "print(model_names_filter_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results[\"Metric\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_sites_models_results_filepaths = sites_models_results_filepaths\n",
    "\n",
    "# looping each model site, and filter combination\n",
    "\n",
    "for model_comb_indx in tqdm(range(model_names_filter_groups.shape[0])):\n",
    "    filter_groups = [model_names_filter_groups.iloc[model_comb_indx, 1]]\n",
    "    model_names = [model_names_filter_groups.iloc[model_comb_indx, 0]]\n",
    "\n",
    "\n",
    "    model_name = model_names[0]\n",
    "\n",
    "    # focusing on \"all\" race results\n",
    "    sites_models_results_filepaths = [\n",
    "        model_path\n",
    "        for model_path in original_sites_models_results_filepaths\n",
    "        if model_name + \"all\" in model_path\n",
    "        and \"no_race\" in model_name\n",
    "        and \"count\" not in model_name\n",
    "    ]\n",
    "\n",
    "    # looping for each model the results, and keeping only the results\n",
    "    # on test data\n",
    "    for models_results_filepath in sites_models_results_filepaths:\n",
    "        model_name = models_results_filepath.replace(results_path, \"\").replace(\n",
    "            \"all_datasets_results.csv\", \"\"\n",
    "        )\n",
    "\n",
    "        df_results = pd.read_csv(models_results_filepath)\n",
    "        df_results = df_results[\n",
    "            df_results[\"Dataset\"].str.contains(\"_test_tested_on\")\n",
    "        ].reset_index(drop=True)\n",
    "\n",
    "    df_results[\"Dataset\"].unique()\n",
    "\n",
    "\n",
    "    # not keeping the optimal threshold row\n",
    "    metrics = [metric for metric in df_results[\"Metric\"].unique() if \"Optimal\" not in metric and \"Prev\" not in metric]\n",
    "\n",
    "    for filter_group in filter_groups:\n",
    "        print(filter_group)\n",
    "\n",
    "        for metric in metrics:\n",
    "\n",
    "            cond = (df_results[\"Race Ethnicity category\"] == filter_group) & (\n",
    "                df_results[\"Dataset\"].str.contains(model_name)\n",
    "            )\n",
    "\n",
    "            df_results_filter_group = df_results[cond].reset_index(drop=True)\n",
    "\n",
    "            print(df_results_filter_group)\n",
    "\n",
    "            # grouping into a d x d df of time\n",
    "            pred_target_res = []\n",
    "\n",
    "            for pred_year in range(1, num_epochs + 1):\n",
    "                pred_year_res = []\n",
    "                for target_year in range(1, num_epochs + 1):\n",
    "                    if pred_year <= target_year:\n",
    "                        values = df_results_filter_group.loc[\n",
    "                            df_results_filter_group[\"Metric\"] == metric,\n",
    "                            \"Prediction Year \"\n",
    "                            + str(pred_year)\n",
    "                            + \", target year \"\n",
    "                            + str(target_year),\n",
    "                        ].reset_index(drop=True)[0]\n",
    "                        pred_year_res.append(values)\n",
    "                    else:\n",
    "                        pred_year_res.append(np.nan)\n",
    "                pred_target_res.append(pred_year_res)\n",
    "\n",
    "            pred_cols = [\"Pred. year \" + str(i) for i in range(1, num_epochs + 1)]\n",
    "            targ_cols = [\"Targ. year \" + str(i) for i in range(1, num_epochs + 1)]\n",
    "\n",
    "            # prevalence vec\n",
    "            pred_targ_cols = [\"Prediction Year \"\n",
    "                            + str(i)\n",
    "                            + \", target year \"\n",
    "                            + str(i) for i in range(1, num_epochs + 1)]\n",
    "            prevalences = df_results_filter_group.loc[\n",
    "                            df_results_filter_group[\"Metric\"] == \"Outcome Prevalence\",pred_targ_cols].values[0]\n",
    "\n",
    "            df_plot = pd.DataFrame(data=pred_target_res, columns=targ_cols, index=pred_cols)\n",
    "\n",
    "            # plots\n",
    "\n",
    "            # Create a mask\n",
    "            mask = ~np.triu(np.ones_like(df_plot, dtype=bool))\n",
    "\n",
    "            mask = np.concatenate((mask,[[True]*6]),axis=0)\n",
    "\n",
    "            prev_series = pd.Series({col:val for col,val in zip(df_plot.columns,prevalences)},name=\"Prev. â‰¥40% eGFR decl. - %\")\n",
    "            # prev_series\n",
    "            df_plot = df_plot.append(prev_series)\n",
    "\n",
    "            mpl.rcParams.update({'font.size': 14})\n",
    "\n",
    "            # set x-axis on top or bottom\n",
    "            plt.rcParams[\"xtick.bottom\"] = plt.rcParams[\"xtick.labelbottom\"] = False\n",
    "            plt.rcParams[\"xtick.top\"] = plt.rcParams[\"xtick.labeltop\"] = True\n",
    "\n",
    "            # Create a custom divergin palette\n",
    "            # https://matplotlib.org/stable/tutorials/colors/colorbar_only.html\n",
    "            cmap = mpl.cm.viridis\n",
    "\n",
    "            plt.figure(figsize=(10, 10))\n",
    "            plt.title(metric + \" over time\")\n",
    "            sns.heatmap(\n",
    "                df_plot,\n",
    "                mask=mask,  # use symbol to reverse mask\n",
    "                center=0,\n",
    "                annot=True,\n",
    "                fmt=\".2f\",\n",
    "                square=True,\n",
    "                cmap=cmap,\n",
    "            )\n",
    "\n",
    "\n",
    "            # labels rotation angle\n",
    "            plt.yticks(rotation=0)\n",
    "            plt.xticks(rotation=45)\n",
    "\n",
    "            # # add annotations with the values of the data\n",
    "            for i in [6]:\n",
    "                for j in range(6):\n",
    "                    text = \"{:.2f}\".format(df_plot.iloc[i,j])\n",
    "                    plt.text(j+0.5, i+0.5, text, ha='center', va='center', color='black')\n",
    "\n",
    "            path_to_write = results_path + \"plots/\" + site + \"/\" + model_name + filter_group.replace(\" \",\"_\")\n",
    "            print(path_to_write)\n",
    "\n",
    "            # os.system('mkdir -p ' + path_to_write)\n",
    "            # plt.savefig(results_path + \"plots/\" + site + \"/\" + model_name + filter_group.replace(\" \",\"_\")  + \"/\" + model_name + \"_\" + filter_group.replace(\" \",\"_\") + \"_\" + metric.replace(\"/\",\"_\") +'.png',pad_inches=1)\n",
    "            # plt.close()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "1760e995f14b06106c87bf32d6451534e77cd9783514cdc2898bf5a99d0d31bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
